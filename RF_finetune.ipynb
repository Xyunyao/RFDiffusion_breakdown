{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c63e814",
   "metadata": {},
   "source": [
    "# This is RFdiffusion finetuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f778780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(\"/home/yunyao/RFdiffusion/models/Base_epoch8_ckpt.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0cf35707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['model_state_dict', 'final_state_dict', 'config_dict'])\n",
      "dict_keys(['model', 'diffuser', 'seq_diffuser', 'preprocess'])\n",
      "dict_keys(['n_extra_block', 'n_main_block', 'n_ref_block', 'd_msa', 'd_msa_full', 'd_pair', 'd_templ', 'n_head_msa', 'n_head_pair', 'n_head_templ', 'd_hidden', 'd_hidden_templ', 'p_drop', 'd_time_emb', 'd_time_emb_proj', 'freeze_track_motif', 'use_motif_timestep', 'SE3_param_full', 'SE3_param_topk'])\n",
      "dict_keys(['sidechain_input', 'motif_sidechain_input', 'sequence_decode', 'd_t1d', 'd_t2d', 'predict_previous', 'prob_self_cond', 'str_self_cond', 'seq_self_cond', 'new_self_cond'])\n"
     ]
    }
   ],
   "source": [
    "print(checkpoint.keys())\n",
    "# Extract the model state dictionary\n",
    "model_state_dict = checkpoint['model_state_dict']\n",
    "# Print the keys of the state dictionary        \n",
    "print(checkpoint['config_dict'].keys())\n",
    "print(checkpoint['config_dict']['model'].keys())\n",
    "print(checkpoint['config_dict']['preprocess'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56fbf9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['self', 'n_extra_block', 'n_main_block', 'n_ref_block', 'd_msa', 'd_msa_full', 'd_pair', 'd_templ', 'n_head_msa', 'n_head_pair', 'n_head_templ', 'd_hidden', 'd_hidden_templ', 'p_drop', 'd_t1d', 'd_t2d', 'T', 'use_motif_timestep', 'freeze_track_motif', 'SE3_param_full', 'SE3_param_topk', 'input_seq_onehot'])\n"
     ]
    }
   ],
   "source": [
    "valid_keys = signature(RoseTTAFoldModule.__init__).parameters.keys()\n",
    "print(valid_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "708bbc9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rfdiffusion.RoseTTAFoldModel import RoseTTAFoldModule\n",
    "from inspect import signature\n",
    "\n",
    "valid_keys = signature(RoseTTAFoldModule.__init__).parameters.keys()\n",
    "filtered_model_config = {}\n",
    "\n",
    "# Iterate through all keys in checkpoint['config_dict']\n",
    "for key in checkpoint['config_dict']:\n",
    "    # Ensure the value is a dictionary to avoid issues\n",
    "    if isinstance(checkpoint['config_dict'][key], dict):\n",
    "        # Filter valid parameters for the current key\n",
    "        filtered_model_config.update(\n",
    "            {k: v for k, v in checkpoint['config_dict'][key].items() if k in valid_keys}\n",
    "        )\n",
    "\n",
    "# Initialize the model\n",
    "model = RoseTTAFoldModule(**filtered_model_config)\n",
    "\n",
    "# Load the model state dictionary into the model\n",
    "model.load_state_dict(model_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75f05312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoseTTAFoldModule(\n",
      "  (latent_emb): MSA_emb(\n",
      "    (emb): Linear(in_features=48, out_features=256, bias=True)\n",
      "    (emb_q): Embedding(22, 256)\n",
      "    (emb_left): Embedding(22, 128)\n",
      "    (emb_right): Embedding(22, 128)\n",
      "    (emb_state): Embedding(22, 64)\n",
      "    (drop): Dropout(p=0.15, inplace=False)\n",
      "    (pos): PositionalEncoding2D(\n",
      "      (emb): Embedding(65, 128)\n",
      "      (drop): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (full_emb): Extra_emb(\n",
      "    (emb): Linear(in_features=25, out_features=64, bias=True)\n",
      "    (emb_q): Embedding(22, 64)\n",
      "    (drop): Dropout(p=0.15, inplace=False)\n",
      "  )\n",
      "  (templ_emb): Templ_emb(\n",
      "    (emb): Linear(in_features=88, out_features=64, bias=True)\n",
      "    (templ_stack): TemplatePairStack(\n",
      "      (block): ModuleList(\n",
      "        (0-1): 2 x PairStr2Pair(\n",
      "          (emb_rbf): Linear(in_features=36, out_features=32, bias=True)\n",
      "          (proj_rbf): Linear(in_features=32, out_features=64, bias=True)\n",
      "          (drop_row): Dropout()\n",
      "          (drop_col): Dropout()\n",
      "          (row_attn): BiasedAxialAttention(\n",
      "            (norm_pair): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_bias): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "            (to_q): Linear(in_features=64, out_features=128, bias=False)\n",
      "            (to_k): Linear(in_features=64, out_features=128, bias=False)\n",
      "            (to_v): Linear(in_features=64, out_features=128, bias=False)\n",
      "            (to_b): Linear(in_features=64, out_features=4, bias=False)\n",
      "            (to_g): Linear(in_features=64, out_features=128, bias=True)\n",
      "            (to_out): Linear(in_features=128, out_features=64, bias=True)\n",
      "          )\n",
      "          (col_attn): BiasedAxialAttention(\n",
      "            (norm_pair): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_bias): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "            (to_q): Linear(in_features=64, out_features=128, bias=False)\n",
      "            (to_k): Linear(in_features=64, out_features=128, bias=False)\n",
      "            (to_v): Linear(in_features=64, out_features=128, bias=False)\n",
      "            (to_b): Linear(in_features=64, out_features=4, bias=False)\n",
      "            (to_g): Linear(in_features=64, out_features=128, bias=True)\n",
      "            (to_out): Linear(in_features=128, out_features=64, bias=True)\n",
      "          )\n",
      "          (ff): FeedForwardLayer(\n",
      "            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=64, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (attn): Attention(\n",
      "      (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
      "      (to_k): Linear(in_features=64, out_features=128, bias=False)\n",
      "      (to_v): Linear(in_features=64, out_features=128, bias=False)\n",
      "      (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (emb_t1d): Linear(in_features=52, out_features=64, bias=True)\n",
      "    (proj_t1d): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (attn_tor): Attention(\n",
      "      (to_q): Linear(in_features=64, out_features=128, bias=False)\n",
      "      (to_k): Linear(in_features=64, out_features=128, bias=False)\n",
      "      (to_v): Linear(in_features=64, out_features=128, bias=False)\n",
      "      (to_out): Linear(in_features=128, out_features=64, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (recycle): Recycling(\n",
      "    (proj_dist): Linear(in_features=164, out_features=128, bias=True)\n",
      "    (norm_state): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm_pair): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm_msa): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (simulator): IterativeSimulator(\n",
      "    (proj_state): Linear(in_features=64, out_features=8, bias=True)\n",
      "    (extra_block): ModuleList(\n",
      "      (0-3): 4 x IterBlock(\n",
      "        (msa2msa): MSAPairStr2MSA(\n",
      "          (norm_pair): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (proj_pair): Linear(in_features=164, out_features=128, bias=True)\n",
      "          (norm_state): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "          (proj_state): Linear(in_features=8, out_features=64, bias=True)\n",
      "          (drop_row): Dropout()\n",
      "          (row_attn): MSARowAttentionWithBias(\n",
      "            (norm_msa): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_pair): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (seq_weight): SequenceWeight(\n",
      "              (to_query): Linear(in_features=64, out_features=64, bias=True)\n",
      "              (to_key): Linear(in_features=64, out_features=64, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (to_k): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (to_v): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (to_b): Linear(in_features=128, out_features=8, bias=False)\n",
      "            (to_g): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (to_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (col_attn): MSAColGlobalAttention(\n",
      "            (norm_msa): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "            (to_q): Linear(in_features=64, out_features=64, bias=False)\n",
      "            (to_k): Linear(in_features=64, out_features=8, bias=False)\n",
      "            (to_v): Linear(in_features=64, out_features=8, bias=False)\n",
      "            (to_g): Linear(in_features=64, out_features=64, bias=True)\n",
      "            (to_out): Linear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (ff): FeedForwardLayer(\n",
      "            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.15, inplace=False)\n",
      "            (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (msa2pair): MSA2Pair(\n",
      "          (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (proj_left): Linear(in_features=64, out_features=16, bias=True)\n",
      "          (proj_right): Linear(in_features=64, out_features=16, bias=True)\n",
      "          (proj_out): Linear(in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (pair2pair): PairStr2Pair(\n",
      "          (emb_rbf): Linear(in_features=36, out_features=32, bias=True)\n",
      "          (proj_rbf): Linear(in_features=32, out_features=128, bias=True)\n",
      "          (drop_row): Dropout()\n",
      "          (drop_col): Dropout()\n",
      "          (row_attn): BiasedAxialAttention(\n",
      "            (norm_pair): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_bias): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (to_k): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (to_v): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (to_b): Linear(in_features=128, out_features=4, bias=False)\n",
      "            (to_g): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (col_attn): BiasedAxialAttention(\n",
      "            (norm_pair): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_bias): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (to_k): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (to_v): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (to_b): Linear(in_features=128, out_features=4, bias=False)\n",
      "            (to_g): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (ff): FeedForwardLayer(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (str2str): Str2Str(\n",
      "          (norm_msa): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm_pair): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm_state): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "          (embed_x): Linear(in_features=72, out_features=8, bias=True)\n",
      "          (embed_e1): Linear(in_features=128, out_features=32, bias=True)\n",
      "          (embed_e2): Linear(in_features=69, out_features=32, bias=True)\n",
      "          (norm_node): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm_edge1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm_edge2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (se3): SE3TransformerWrapper(\n",
      "            (se3): SE3Transformer(\n",
      "              (graph_modules): Sequential(\n",
      "                (0): AttentionBlockSE3(\n",
      "                  (to_key_value): ConvSE3(\n",
      "                    (conv_in): ModuleDict(\n",
      "                      (1): VersatileConvSE3(\n",
      "                        (radial_func): RadialProfile(\n",
      "                          (net): Sequential(\n",
      "                            (0): Linear(in_features=33, out_features=32, bias=True)\n",
      "                            (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                            (2): ReLU()\n",
      "                            (3): Linear(in_features=32, out_features=32, bias=True)\n",
      "                            (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                            (5): ReLU()\n",
      "                            (6): Linear(in_features=32, out_features=192, bias=False)\n",
      "                          )\n",
      "                        )\n",
      "                      )\n",
      "                      (0): VersatileConvSE3(\n",
      "                        (radial_func): RadialProfile(\n",
      "                          (net): Sequential(\n",
      "                            (0): Linear(in_features=33, out_features=32, bias=True)\n",
      "                            (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                            (2): ReLU()\n",
      "                            (3): Linear(in_features=32, out_features=32, bias=True)\n",
      "                            (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                            (5): ReLU()\n",
      "                            (6): Linear(in_features=32, out_features=256, bias=False)\n",
      "                          )\n",
      "                        )\n",
      "                      )\n",
      "                    )\n",
      "                  )\n",
      "                  (to_query): LinearSE3(\n",
      "                    (weights): ParameterDict(\n",
      "                        (0): Parameter containing: [torch.FloatTensor of size 8x8]\n",
      "                        (1): Parameter containing: [torch.FloatTensor of size 8x3]\n",
      "                    )\n",
      "                  )\n",
      "                  (attention): AttentionSE3()\n",
      "                  (project): LinearSE3(\n",
      "                    (weights): ParameterDict(\n",
      "                        (0): Parameter containing: [torch.FloatTensor of size 32x16]\n",
      "                        (1): Parameter containing: [torch.FloatTensor of size 32x11]\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (1): NormSE3(\n",
      "                  (nonlinearity): ReLU()\n",
      "                  (group_norm): GroupNorm(2, 64, eps=1e-05, affine=True)\n",
      "                )\n",
      "                (2): ConvSE3(\n",
      "                  (conv_out): ModuleDict(\n",
      "                    (1): VersatileConvSE3(\n",
      "                      (radial_func): RadialProfile(\n",
      "                        (net): Sequential(\n",
      "                          (0): Linear(in_features=33, out_features=32, bias=True)\n",
      "                          (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                          (2): ReLU()\n",
      "                          (3): Linear(in_features=32, out_features=32, bias=True)\n",
      "                          (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                          (5): ReLU()\n",
      "                          (6): Linear(in_features=32, out_features=256, bias=False)\n",
      "                        )\n",
      "                      )\n",
      "                    )\n",
      "                    (0): VersatileConvSE3(\n",
      "                      (radial_func): RadialProfile(\n",
      "                        (net): Sequential(\n",
      "                          (0): Linear(in_features=33, out_features=32, bias=True)\n",
      "                          (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                          (2): ReLU()\n",
      "                          (3): Linear(in_features=32, out_features=32, bias=True)\n",
      "                          (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                          (5): ReLU()\n",
      "                          (6): Linear(in_features=32, out_features=512, bias=False)\n",
      "                        )\n",
      "                      )\n",
      "                    )\n",
      "                  )\n",
      "                  (to_kernel_self): ParameterDict(\n",
      "                      (1): Parameter containing: [torch.FloatTensor of size 2x32]\n",
      "                      (0): Parameter containing: [torch.FloatTensor of size 8x32]\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (sc_predictor): SCPred(\n",
      "            (norm_s0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_si): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear_s0): Linear(in_features=64, out_features=128, bias=True)\n",
      "            (linear_si): Linear(in_features=8, out_features=128, bias=True)\n",
      "            (linear_1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (linear_2): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (linear_3): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (linear_4): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (linear_out): Linear(in_features=128, out_features=20, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (main_block): ModuleList(\n",
      "      (0-31): 32 x IterBlock(\n",
      "        (msa2msa): MSAPairStr2MSA(\n",
      "          (norm_pair): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (proj_pair): Linear(in_features=164, out_features=128, bias=True)\n",
      "          (norm_state): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "          (proj_state): Linear(in_features=8, out_features=256, bias=True)\n",
      "          (drop_row): Dropout()\n",
      "          (row_attn): MSARowAttentionWithBias(\n",
      "            (norm_msa): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_pair): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (seq_weight): SequenceWeight(\n",
      "              (to_query): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (to_key): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (to_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (to_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (to_b): Linear(in_features=128, out_features=8, bias=False)\n",
      "            (to_g): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (to_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (col_attn): MSAColAttention(\n",
      "            (norm_msa): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (to_q): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (to_k): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (to_v): Linear(in_features=256, out_features=256, bias=False)\n",
      "            (to_g): Linear(in_features=256, out_features=256, bias=True)\n",
      "            (to_out): Linear(in_features=256, out_features=256, bias=True)\n",
      "          )\n",
      "          (ff): FeedForwardLayer(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "            (dropout): Dropout(p=0.15, inplace=False)\n",
      "            (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (msa2pair): MSA2Pair(\n",
      "          (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (proj_left): Linear(in_features=256, out_features=16, bias=True)\n",
      "          (proj_right): Linear(in_features=256, out_features=16, bias=True)\n",
      "          (proj_out): Linear(in_features=256, out_features=128, bias=True)\n",
      "        )\n",
      "        (pair2pair): PairStr2Pair(\n",
      "          (emb_rbf): Linear(in_features=36, out_features=32, bias=True)\n",
      "          (proj_rbf): Linear(in_features=32, out_features=128, bias=True)\n",
      "          (drop_row): Dropout()\n",
      "          (drop_col): Dropout()\n",
      "          (row_attn): BiasedAxialAttention(\n",
      "            (norm_pair): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_bias): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (to_k): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (to_v): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (to_b): Linear(in_features=128, out_features=4, bias=False)\n",
      "            (to_g): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (col_attn): BiasedAxialAttention(\n",
      "            (norm_pair): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_bias): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (to_k): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (to_v): Linear(in_features=128, out_features=128, bias=False)\n",
      "            (to_b): Linear(in_features=128, out_features=4, bias=False)\n",
      "            (to_g): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (to_out): Linear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (ff): FeedForwardLayer(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (str2str): Str2Str(\n",
      "          (norm_msa): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm_pair): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm_state): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "          (embed_x): Linear(in_features=264, out_features=8, bias=True)\n",
      "          (embed_e1): Linear(in_features=128, out_features=32, bias=True)\n",
      "          (embed_e2): Linear(in_features=69, out_features=32, bias=True)\n",
      "          (norm_node): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm_edge1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm_edge2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "          (se3): SE3TransformerWrapper(\n",
      "            (se3): SE3Transformer(\n",
      "              (graph_modules): Sequential(\n",
      "                (0): AttentionBlockSE3(\n",
      "                  (to_key_value): ConvSE3(\n",
      "                    (conv_in): ModuleDict(\n",
      "                      (1): VersatileConvSE3(\n",
      "                        (radial_func): RadialProfile(\n",
      "                          (net): Sequential(\n",
      "                            (0): Linear(in_features=33, out_features=32, bias=True)\n",
      "                            (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                            (2): ReLU()\n",
      "                            (3): Linear(in_features=32, out_features=32, bias=True)\n",
      "                            (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                            (5): ReLU()\n",
      "                            (6): Linear(in_features=32, out_features=192, bias=False)\n",
      "                          )\n",
      "                        )\n",
      "                      )\n",
      "                      (0): VersatileConvSE3(\n",
      "                        (radial_func): RadialProfile(\n",
      "                          (net): Sequential(\n",
      "                            (0): Linear(in_features=33, out_features=32, bias=True)\n",
      "                            (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                            (2): ReLU()\n",
      "                            (3): Linear(in_features=32, out_features=32, bias=True)\n",
      "                            (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                            (5): ReLU()\n",
      "                            (6): Linear(in_features=32, out_features=256, bias=False)\n",
      "                          )\n",
      "                        )\n",
      "                      )\n",
      "                    )\n",
      "                  )\n",
      "                  (to_query): LinearSE3(\n",
      "                    (weights): ParameterDict(\n",
      "                        (0): Parameter containing: [torch.FloatTensor of size 8x8]\n",
      "                        (1): Parameter containing: [torch.FloatTensor of size 8x3]\n",
      "                    )\n",
      "                  )\n",
      "                  (attention): AttentionSE3()\n",
      "                  (project): LinearSE3(\n",
      "                    (weights): ParameterDict(\n",
      "                        (0): Parameter containing: [torch.FloatTensor of size 32x16]\n",
      "                        (1): Parameter containing: [torch.FloatTensor of size 32x11]\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (1): NormSE3(\n",
      "                  (nonlinearity): ReLU()\n",
      "                  (group_norm): GroupNorm(2, 64, eps=1e-05, affine=True)\n",
      "                )\n",
      "                (2): ConvSE3(\n",
      "                  (conv_out): ModuleDict(\n",
      "                    (1): VersatileConvSE3(\n",
      "                      (radial_func): RadialProfile(\n",
      "                        (net): Sequential(\n",
      "                          (0): Linear(in_features=33, out_features=32, bias=True)\n",
      "                          (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                          (2): ReLU()\n",
      "                          (3): Linear(in_features=32, out_features=32, bias=True)\n",
      "                          (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                          (5): ReLU()\n",
      "                          (6): Linear(in_features=32, out_features=256, bias=False)\n",
      "                        )\n",
      "                      )\n",
      "                    )\n",
      "                    (0): VersatileConvSE3(\n",
      "                      (radial_func): RadialProfile(\n",
      "                        (net): Sequential(\n",
      "                          (0): Linear(in_features=33, out_features=32, bias=True)\n",
      "                          (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                          (2): ReLU()\n",
      "                          (3): Linear(in_features=32, out_features=32, bias=True)\n",
      "                          (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                          (5): ReLU()\n",
      "                          (6): Linear(in_features=32, out_features=512, bias=False)\n",
      "                        )\n",
      "                      )\n",
      "                    )\n",
      "                  )\n",
      "                  (to_kernel_self): ParameterDict(\n",
      "                      (1): Parameter containing: [torch.FloatTensor of size 2x32]\n",
      "                      (0): Parameter containing: [torch.FloatTensor of size 8x32]\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (sc_predictor): SCPred(\n",
      "            (norm_s0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm_si): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "            (linear_s0): Linear(in_features=256, out_features=128, bias=True)\n",
      "            (linear_si): Linear(in_features=8, out_features=128, bias=True)\n",
      "            (linear_1): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (linear_2): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (linear_3): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (linear_4): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (linear_out): Linear(in_features=128, out_features=20, bias=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (proj_state2): Linear(in_features=8, out_features=64, bias=True)\n",
      "    (str_refiner): Str2Str(\n",
      "      (norm_msa): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm_pair): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm_state): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (embed_x): Linear(in_features=320, out_features=64, bias=True)\n",
      "      (embed_e1): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (embed_e2): Linear(in_features=101, out_features=64, bias=True)\n",
      "      (norm_node): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm_edge1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm_edge2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (se3): SE3TransformerWrapper(\n",
      "        (se3): SE3Transformer(\n",
      "          (graph_modules): Sequential(\n",
      "            (0): AttentionBlockSE3(\n",
      "              (to_key_value): ConvSE3(\n",
      "                (conv_in): ModuleDict(\n",
      "                  (1): VersatileConvSE3(\n",
      "                    (radial_func): RadialProfile(\n",
      "                      (net): Sequential(\n",
      "                        (0): Linear(in_features=65, out_features=32, bias=True)\n",
      "                        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                        (2): ReLU()\n",
      "                        (3): Linear(in_features=32, out_features=32, bias=True)\n",
      "                        (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                        (5): ReLU()\n",
      "                        (6): Linear(in_features=32, out_features=192, bias=False)\n",
      "                      )\n",
      "                    )\n",
      "                  )\n",
      "                  (0): VersatileConvSE3(\n",
      "                    (radial_func): RadialProfile(\n",
      "                      (net): Sequential(\n",
      "                        (0): Linear(in_features=65, out_features=32, bias=True)\n",
      "                        (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                        (2): ReLU()\n",
      "                        (3): Linear(in_features=32, out_features=32, bias=True)\n",
      "                        (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                        (5): ReLU()\n",
      "                        (6): Linear(in_features=32, out_features=2048, bias=False)\n",
      "                      )\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (to_query): LinearSE3(\n",
      "                (weights): ParameterDict(\n",
      "                    (0): Parameter containing: [torch.FloatTensor of size 8x64]\n",
      "                    (1): Parameter containing: [torch.FloatTensor of size 8x3]\n",
      "                )\n",
      "              )\n",
      "              (attention): AttentionSE3()\n",
      "              (project): LinearSE3(\n",
      "                (weights): ParameterDict(\n",
      "                    (0): Parameter containing: [torch.FloatTensor of size 32x72]\n",
      "                    (1): Parameter containing: [torch.FloatTensor of size 32x11]\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): NormSE3(\n",
      "              (nonlinearity): ReLU()\n",
      "              (group_norm): GroupNorm(2, 64, eps=1e-05, affine=True)\n",
      "            )\n",
      "            (2): ConvSE3(\n",
      "              (conv_out): ModuleDict(\n",
      "                (1): VersatileConvSE3(\n",
      "                  (radial_func): RadialProfile(\n",
      "                    (net): Sequential(\n",
      "                      (0): Linear(in_features=65, out_features=32, bias=True)\n",
      "                      (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                      (2): ReLU()\n",
      "                      (3): Linear(in_features=32, out_features=32, bias=True)\n",
      "                      (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                      (5): ReLU()\n",
      "                      (6): Linear(in_features=32, out_features=256, bias=False)\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "                (0): VersatileConvSE3(\n",
      "                  (radial_func): RadialProfile(\n",
      "                    (net): Sequential(\n",
      "                      (0): Linear(in_features=65, out_features=32, bias=True)\n",
      "                      (1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                      (2): ReLU()\n",
      "                      (3): Linear(in_features=32, out_features=32, bias=True)\n",
      "                      (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "                      (5): ReLU()\n",
      "                      (6): Linear(in_features=32, out_features=4096, bias=False)\n",
      "                    )\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (to_kernel_self): ParameterDict(\n",
      "                  (1): Parameter containing: [torch.FloatTensor of size 2x32]\n",
      "                  (0): Parameter containing: [torch.FloatTensor of size 64x32]\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (sc_predictor): SCPred(\n",
      "        (norm_s0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm_si): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (linear_s0): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (linear_si): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (linear_1): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (linear_2): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (linear_3): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (linear_4): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (linear_out): Linear(in_features=128, out_features=20, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (c6d_pred): DistanceNetwork(\n",
      "    (proj_symm): Linear(in_features=128, out_features=74, bias=True)\n",
      "    (proj_asymm): Linear(in_features=128, out_features=56, bias=True)\n",
      "  )\n",
      "  (aa_pred): MaskedTokenNetwork(\n",
      "    (proj): Linear(in_features=256, out_features=21, bias=True)\n",
      "  )\n",
      "  (lddt_pred): LDDTNetwork(\n",
      "    (proj): Linear(in_features=64, out_features=50, bias=True)\n",
      "  )\n",
      "  (exp_pred): ExpResolvedNetwork(\n",
      "    (norm_msa): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm_state): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (proj): Linear(in_features=320, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d64495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let us freeze certain layers\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SE3nv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
