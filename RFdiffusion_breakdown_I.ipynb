{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFDiffusion Breakdown\n",
    "Forward add noising: add 3D Gaussian noise to Ca coordinates, use Brownian motion to add noise to manifold of rotation matrix representing N-CA-C orientation   \n",
    "Backward denoising: using RosettaFold as denoising engine (take coorindates with noise as input into RosettaFold and output the 'right' structure )  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Modules\n",
    "Embedding module:\n",
    "1. positionalEmbedding: relative postion encoding [B, L, L, d]\n",
    "2. class MSA_emb: input MSA [B,N,L,d_init] Seq [B,L]          \n",
    "output: msa_embedding :[B,N,L,d_msa] pair_embedding: [B,L,L,d_pair]\n",
    "3. extra_emb: embedding for extra msa\n",
    "4. TemplatePairStack: use PairStr2Pair module to embed template structure [L, L, d]\n",
    "5. TemplateTorsionStack: use MLP+ AttionwithBias to embed torsion\n",
    "6. Templ_emb: embed features including\n",
    "    2d: 37 distogram bin (rbf) +6 orientation  \n",
    "        mask (missing/unaligned) 1\n",
    "    1d: tiled AA (20 +GAP); confidence: 1; contacting or not (1)\n",
    "\n",
    "7. recycleing: take MSA Pair state as input and update them with a engine that do not update parameteres\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention module\n",
    "\n",
    "1. FeedForwardLayer\n",
    "2. Attention (conventional)\n",
    "3. AttentionwithBias: adding bias and gate on top of 2\n",
    "4. SequenceWeight\n",
    "5. MSARowAttentionwithBias\n",
    "6. MSAColAttention\n",
    "7. MSAGlobalAttention: share key value accoress all attention heads\n",
    "8. BaisedAxialAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Track_module (similar to Evoformer)\n",
    "#1. MSA -> MSA update (biased attention. bias from pair & structure)  \n",
    "#2. Pair -> Pair update (biased attention. bias from structure)  \n",
    "#3. MSA -> Pair update (extract coevolution signal)  \n",
    "#4. Str -> Str update (node from MSA, edge from Pair)  \n",
    "\n",
    "Class MSAPairStr2MSA:  \n",
    "        '''  \n",
    "        Inputs:  \n",
    "            - msa: MSA feature (B, N, L, d_msa)  \n",
    "            - pair: Pair feature (B, L, L, d_pair)  \n",
    "            - rbf_feat: Ca-Ca distance feature calculated from xyz coordinates (B, L, L, 36)  \n",
    "            - xyz: xyz coordinates (B, L, n_atom, 3)  \n",
    "            - state: updated node features after SE(3)-Transformer layer (B, L, d_state)  \n",
    "        Output:  \n",
    "            - msa: Updated MSA feature (B, N, L, d_msa)  \n",
    "\n",
    "Class PairStr2Pair:\n",
    "        inputs: pair [B, L, L, d_pair] \n",
    "                rbf_feat [B, L, L, d]\n",
    "\n",
    "        operation:   row/ colum/ff, using rbf_feat as bias\n",
    "        outputs: updated pair \n",
    "\n",
    "Class MSA2Pair: (alphafold2 algorithm 10)\n",
    "        usign outer proecut mean to generate pair represenation from MSA\n",
    "        Key step to flow MSA info into Pair representation\n",
    "\n",
    "Class SCPred: (unique in RosttaFold)\n",
    "        '''\n",
    "        Predict side-chain torsion angles along with backbone torsions\n",
    "        Inputs:\n",
    "            - seq: hidden embeddings corresponding to query sequence (B, L, d_msa)\n",
    "            - state: state feature (output l0 feature) from previous SE3 layer (B, L, d_state)\n",
    "        engine: Many layer of residual connected MLP\n",
    "        Outputs:\n",
    "            - si: predicted torsion angles (phi, psi, omega, chi1~4 with cos/sin, Cb bend, Cb twist, CG) (B, L, 10, 2)\n",
    "        '''\n",
    "Class Str2Str: we breaddown with more details below.\n",
    "\n",
    "Class IterBlock: conbine msa2msa msa2pair pair2pair str2str to build a basic iteration block\n",
    "\n",
    "Class IterativeSimulator: use IterBlock and initial condition/values to update msa, pair, ratation, translation paramter, alpha and state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Str2Str(nn.Module):\n",
    "    def __init__(self, d_msa=256, d_pair=128, d_state=16, \n",
    "            SE3_param={'l0_in_features':32, 'l0_out_features':16, 'num_edge_features':32}, p_drop=0.1):\n",
    "        super(Str2Str, self).__init__()\n",
    "        \n",
    "        # initial node & pair feature process\n",
    "        self.norm_msa = nn.LayerNorm(d_msa) \n",
    "        self.norm_pair = nn.LayerNorm(d_pair)\n",
    "        self.norm_state = nn.LayerNorm(d_state)\n",
    "    \n",
    "        self.embed_x = nn.Linear(d_msa+d_state, SE3_param['l0_in_features'])\n",
    "        self.embed_e1 = nn.Linear(d_pair, SE3_param['num_edge_features'])\n",
    "        self.embed_e2 = nn.Linear(SE3_param['num_edge_features']+36+1, SE3_param['num_edge_features'])\n",
    "        \n",
    "        self.norm_node = nn.LayerNorm(SE3_param['l0_in_features'])\n",
    "        self.norm_edge1 = nn.LayerNorm(SE3_param['num_edge_features'])\n",
    "        self.norm_edge2 = nn.LayerNorm(SE3_param['num_edge_features'])\n",
    "        \n",
    "        self.se3 = SE3TransformerWrapper(**SE3_param)\n",
    "        self.sc_predictor = SCPred(d_msa=d_msa, d_state=SE3_param['l0_out_features'],\n",
    "                                   p_drop=p_drop)\n",
    "        \n",
    "        self.reset_parameter()\n",
    "\n",
    "    def reset_parameter(self):\n",
    "        # initialize weights to normal distribution\n",
    "        self.embed_x = init_lecun_normal(self.embed_x)\n",
    "        self.embed_e1 = init_lecun_normal(self.embed_e1)\n",
    "        self.embed_e2 = init_lecun_normal(self.embed_e2)\n",
    "\n",
    "        # initialize bias to zeros\n",
    "        nn.init.zeros_(self.embed_x.bias)\n",
    "        nn.init.zeros_(self.embed_e1.bias)\n",
    "        nn.init.zeros_(self.embed_e2.bias)\n",
    "    \n",
    "    @torch.cuda.amp.autocast(enabled=False)\n",
    "    def forward(self, msa, pair, R_in, T_in, xyz, state, idx, motif_mask, top_k=64, eps=1e-5):\n",
    "        B, N, L = msa.shape[:3]\n",
    "\n",
    "        if motif_mask is None:\n",
    "            motif_mask = torch.zeros(L).bool()\n",
    "        \n",
    "        # process msa & pair features\n",
    "        node = self.norm_msa(msa[:,0]) # use only query seq represenation\n",
    "        pair = self.norm_pair(pair)\n",
    "        state = self.norm_state(state)\n",
    "       \n",
    "        node = torch.cat((node, state), dim=-1) # add state and seq representation\n",
    "        node = self.norm_node(self.embed_x(node)) # update node embedding\n",
    "        pair = self.norm_edge1(self.embed_e1(pair)) # undate edge embedding\n",
    "        \n",
    "        neighbor = get_seqsep(idx) # get neighhours seq representation\n",
    "        rbf_feat = rbf(torch.cdist(xyz[:,:,1], xyz[:,:,1])) #generate RBF feature based Euclidean distance \n",
    "        pair = torch.cat((pair, rbf_feat, neighbor), dim=-1) # use oriingal pair represenation, rbf feature, and neighours' feature to update edge \n",
    "        pair = self.norm_edge2(self.embed_e2(pair))\n",
    "        \n",
    "        # define graph\n",
    "        if top_k != 0:\n",
    "            G, edge_feats = make_topk_graph(xyz[:,:,1,:], pair, idx, top_k=top_k)\n",
    "        else:\n",
    "            G, edge_feats = make_full_graph(xyz[:,:,1,:], pair, idx, top_k=top_k)\n",
    "        l1_feats = xyz - xyz[:,:,1,:].unsqueeze(2)\n",
    "        l1_feats = l1_feats.reshape(B*L, -1, 3)\n",
    "        \n",
    "        # apply SE(3) Transformer & update coordinates\n",
    "        shift = self.se3(G, node.reshape(B*L, -1, 1), l1_feats, edge_feats)\n",
    "\n",
    "        state = shift['0'].reshape(B, L, -1) # (B, L, C)\n",
    "        \n",
    "        offset = shift['1'].reshape(B, L, 2, 3)\n",
    "        offset[:,motif_mask,...] = 0            # NOTE: motif mask is all zeros if not freeezing the motif \n",
    "\n",
    "        delTi = offset[:,:,0,:] / 10.0 # translation\n",
    "        R = offset[:,:,1,:] / 100.0 # rotation\n",
    "        \n",
    "        Qnorm = torch.sqrt( 1 + torch.sum(R*R, dim=-1) )\n",
    "        qA, qB, qC, qD = 1/Qnorm, R[:,:,0]/Qnorm, R[:,:,1]/Qnorm, R[:,:,2]/Qnorm\n",
    "\n",
    "        delRi = torch.zeros((B,L,3,3), device=xyz.device)\n",
    "        delRi[:,:,0,0] = qA*qA+qB*qB-qC*qC-qD*qD\n",
    "        delRi[:,:,0,1] = 2*qB*qC - 2*qA*qD\n",
    "        delRi[:,:,0,2] = 2*qB*qD + 2*qA*qC\n",
    "        delRi[:,:,1,0] = 2*qB*qC + 2*qA*qD\n",
    "        delRi[:,:,1,1] = qA*qA-qB*qB+qC*qC-qD*qD\n",
    "        delRi[:,:,1,2] = 2*qC*qD - 2*qA*qB\n",
    "        delRi[:,:,2,0] = 2*qB*qD - 2*qA*qC\n",
    "        delRi[:,:,2,1] = 2*qC*qD + 2*qA*qB\n",
    "        delRi[:,:,2,2] = qA*qA-qB*qB-qC*qC+qD*qD\n",
    "\n",
    "        Ri = einsum('bnij,bnjk->bnik', delRi, R_in)\n",
    "        Ti = delTi + T_in #einsum('bnij,bnj->bni', delRi, T_in) + delTi\n",
    "            \n",
    "        alpha = self.sc_predictor(msa[:,0], state)\n",
    "        return Ri, Ti, state, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SE3nv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
