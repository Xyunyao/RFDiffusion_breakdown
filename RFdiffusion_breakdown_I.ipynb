{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFDiffusion Breakdown\n",
    "Forward add noising: add 3D Gaussian noise to Ca coordinates, use Brownian motion to add noise to manifold of rotation matrix representing N-CA-C orientation   \n",
    "Backward denoising: using RosettaFold as denoising engine (take coorindates with noise as input into RosettaFold and output the 'right' structure )  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Modules\n",
    "Embedding module:\n",
    "1. positionalEmbedding: relative postion encoding [B, L, L, d]\n",
    "2. class MSA_emb: input MSA [B,N,L,d_init] Seq [B,L]          \n",
    "output: msa_embedding :[B,N,L,d_msa] pair_embedding: [B,L,L,d_pair]\n",
    "3. extra_emb: embedding for extra msa\n",
    "4. TemplatePairStack: use PairStr2Pair module to embed template structure [L, L, d]\n",
    "5. TemplateTorsionStack: use MLP+ AttionwithBias to embed torsion\n",
    "6. Templ_emb: embed features including\n",
    "    2d: 37 distogram bin (rbf) +6 orientation  \n",
    "        mask (missing/unaligned) 1\n",
    "    1d: tiled AA (20 +GAP); confidence: 1; contacting or not (1)\n",
    "\n",
    "7. recycleing: take MSA Pair state as input and update them with a engine that do not update parameteres\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention module\n",
    "\n",
    "1. FeedForwardLayer\n",
    "2. Attention (conventional)\n",
    "3. AttentionwithBias: adding bias and gate on top of 2\n",
    "4. SequenceWeight\n",
    "5. MSARowAttentionwithBias\n",
    "6. MSAColAttention\n",
    "7. MSAGlobalAttention: share key value accoress all attention heads\n",
    "8. BaisedAxialAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Track_module (similar to Evoformer)\n",
    "#1. MSA -> MSA update (biased attention. bias from pair & structure)\n",
    "#2. Pair -> Pair update (biased attention. bias from structure)\n",
    "#3. MSA -> Pair update (extract coevolution signal)\n",
    "#4. Str -> Str update (node from MSA, edge from Pair)\n",
    "\n",
    "Class MSAPairStr2MSA:\n",
    "        '''\n",
    "        Inputs:\n",
    "            - msa: MSA feature (B, N, L, d_msa)\n",
    "            - pair: Pair feature (B, L, L, d_pair)\n",
    "            - rbf_feat: Ca-Ca distance feature calculated from xyz coordinates (B, L, L, 36)\n",
    "            - xyz: xyz coordinates (B, L, n_atom, 3)\n",
    "            - state: updated node features after SE(3)-Transformer layer (B, L, d_state)\n",
    "        Output:\n",
    "            - msa: Updated MSA feature (B, N, L, d_msa)\n",
    "\n",
    "Class PairStr2Pair:\n",
    "        inputs: pair [B, L, L, d_pair] \n",
    "                rbf_feat [B, L, L, d]\n",
    "\n",
    "        operation:   row/ colum/ff, using rbf_feat as bias\n",
    "        outputs: updated pair \n",
    "\n",
    "Class MSA2Pair:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SE3nv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
